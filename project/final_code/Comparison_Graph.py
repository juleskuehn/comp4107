# Code based on
# https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.lines import Line2D

# Load validation accuracies from files
# Naive models:
naive_mlp = [0.09047114365545814, 0.09080836304075537, 0.09042296945755854, 0.08970035648906445, 0.09047114365545814, 0.0900857500722613, 0.09080836304075537, 0.09042296945755854, 0.09100105983235379, 0.09076018884285576, 0.09095288563445418, 0.09056749205125735, 0.08960400809326524, 0.09032662106175932, 0.08984487908276327, 0.08960400809326524, 0.09066384044705655, 0.09133827921765103, 0.09157915020714905, 0.09133827921765103, 0.09032662106175932, 0.0900857500722613, 0.09056749205125735, 0.09076018884285576, 0.09080836304075537]
naive_cnn = [0.08974853068696406, 0.08661720782348974, 0.08965218229116485, 0.09191636959244628, 0.09533673764331824, 0.09673378938240679, 0.0978417959340977, 0.09962424125638308, 0.100732247808074, 0.1017439059639657, 0.10212929954716254, 0.10318913190095384, 0.10357452548415069, 0.10492340302533963, 0.10424896425474516, 0.10424896425474516, 0.10453800944214278, 0.10713941612872145, 0.10699489353502264, 0.10834377107621158, 0.10925908083630408, 0.10983717121109933, 0.11128239714808748, 0.11108970035648906, 0.11176413912708354]

# Embedding trained from scratch:
mlp = [0.5260140668801437, 0.5816552654570588, 0.5980826668948817, 0.6027073899363148, 0.6090182098640344, 0.6120050101050959, 0.611956835918682, 0.6133538876577705, 0.6155217265804812, 0.6175450428750361, 0.6177859138645342, 0.6202909721696707, 0.6209654109402651, 0.6195683592011766, 0.6183640042536864, 0.6186048752288275, 0.6213026303112054, 0.6192311398158793, 0.6213508045234619, 0.6180267848540322, 0.620483668946912, 0.6183640042393295, 0.6184121784544575, 0.6207727141343097, 0.620483668946912]
cnn = [0.5288563445303778, 0.583293188139703, 0.5999132864322951, 0.6103670873937378, 0.6120531843374524, 0.6165333847421157, 0.6198092301820604, 0.6227960304690643, 0.6207727141400525, 0.619375662400964, 0.6235668176182295, 0.6212062819211489, 0.6202427979631568, 0.6194238365988636, 0.6213508045148477, 0.6199537527757591, 0.6211581076887924, 0.6197610559841608, 0.6204836689641404, 0.6193756623837355, 0.6193274881858359, 0.620339146358956, 0.6186530494152415, 0.6186530494152415, 0.6190866171963378]
simple = [0.4894016764506013, 0.5806917815048096, 0.6069948935407654, 0.619375662400964, 0.62838423740819, 0.6287214567762589, 0.6307929473031703, 0.631515560254436, 0.6297331149321506, 0.6288659793699577, 0.6284805857867608, 0.6269390114539735, 0.6243857789825229, 0.6248675209615189, 0.6225551594623379, 0.62332594664596, 0.6226033336774659, 0.61985740437996, 0.6188457462240683, 0.6184603526408714, 0.6155699007668951, 0.6149918103920998, 0.6148954619963006, 0.6130166682782161, 0.6143173716215055]

# Embedding initialized to GloVe
glove_simple = [0.49845842566721266, 0.4995182580095183, 0.5017342711301286, 0.5013488775297034, 0.49966278062044556, 0.5002890451931404, 0.49706137389941, 0.4973504191040361, 0.49667598035067007, 0.4955679737645223, 0.4952789285943531, 0.49537527702460915, 0.4938818768897214, 0.4943636188342606, 0.4954716254204084, 0.49026881204725103, 0.49272569612290246, 0.49378552845946533, 0.49291839293172934, 0.4926293477443317, 0.48906445706530405, 0.48959437327665656, 0.48997976684262495, 0.49181038634558155, 0.4904615088388495]
glove_mlp = [0.493833702686079, 0.5077560458790649, 0.51951055014934, 0.5261585894767139, 0.5270257250389069, 0.5321803641969362, 0.5337219385469519, 0.5371904807957235, 0.536997784004125, 0.5393583197012057, 0.5371904807784951, 0.5385875324888695, 0.5439348684844397, 0.544079391095367, 0.5421042489527691, 0.5384430099238847, 0.5392619712766924, 0.5395028422661905, 0.5373831775844505, 0.5418633779747567, 0.5383948356972711, 0.5371904807670094, 0.5386357067327117, 0.5382021389516152, 0.5368532613931978]
glove_cnn = [0.5703825031485513, 0.5904229694288444, 0.600973118768858, 0.6127757973002037, 0.6139319780038517, 0.6172078234610249, 0.6204836689641404, 0.6244821273438652, 0.6270353598325442, 0.6243376047846232, 0.6241449079585679, 0.6280470180056644, 0.6260718759032662, 0.6268426630409458, 0.6279988438077648, 0.6300703343002194, 0.6293958955296249, 0.6274689276136407, 0.6274207534444551, 0.6281915406280772, 0.6273244050658844, 0.6260718759204946, 0.6295885923499374, 0.6274207534444551, 0.6291550245860693]
glove_train_simple = [0.5417188553982863, 0.5878697369401654, 0.6097889970304282, 0.6253010887253869, 0.6324308699973, 0.6382599479431522, 0.6420175353965498, 0.6414394450217545, 0.6471721745718075, 0.6461605164159159, 0.64688312938441, 0.6456306002390202, 0.6464013873881854, 0.646449561586085, 0.6451970324406954, 0.6446189420659001, 0.6470758261587799, 0.6469794777629807, 0.6439926774932052, 0.6439926774932052, 0.6408131804318312, 0.6406204836574613, 0.6405723094423332, 0.6411985740322566, 0.6403796126507348]
glove_train_mlp = [0.5111282397148087, 0.5599768763562941, 0.5816070912476736, 0.5949513440658643, 0.6080065517138856, 0.610222564800039, 0.6234222950245307, 0.6255419597321134, 0.6278543212312944, 0.6319973022506604, 0.633201657215379, 0.636814722040621, 0.6385489931822351, 0.6424029290142036, 0.6400905675150225, 0.6444744195238865, 0.6427883225974004, 0.6420175354310067, 0.639849696479582, 0.6432218903325543, 0.6401387416669796, 0.6442817226863456, 0.6467386067792255, 0.647653916539318, 0.6455342518317354]
glove_train_cnn = [0.5699489353559692, 0.603140957708797, 0.6205318431505544, 0.6261682242875798, 0.6318046054246051, 0.6378745543771838, 0.6425474515562171, 0.6449079872705261, 0.6477502649466028, 0.6482320069255989, 0.6513151555911736, 0.6517005491743705, 0.6534829944794274, 0.6520377685424392, 0.6541092590521222, 0.6546391752290179, 0.6555544850063388, 0.6575778013181223, 0.6581077174950178, 0.6586376336719135, 0.6566143173429017, 0.657240581932825, 0.65540996241264, 0.6574332787416518, 0.6573369303286242]

def plot_all_models_training():
    # Plot training & validation accuracy values
    ax = plt.gca()
    for result in [naive_mlp, naive_cnn, simple, mlp, cnn, glove_simple, glove_mlp, glove_cnn, glove_train_simple, glove_train_mlp, glove_train_cnn]:
        color = next(ax._get_lines.prop_cycler)['color']
        plt.plot(result, color=color)
        # Show max
        y = max(result)
        x = result.index(y)
        plt.plot([x], [y], marker='o', markersize=10, color=color)
        ax.annotate(f"{y:.3f}", (x, y-0.05), ha='center', bbox=dict(boxstyle="round", fc="w", alpha=1))

    plt.ylim(0, 1)
    plt.xticks([epoch for epoch in range(0, 25)])
    plt.title('Model accuracy')
    plt.ylabel('Validation Accuracy')
    plt.xlabel('Epoch')
    plt.gca().set_prop_cycle(None)
    custom_lines = [Line2D([0], [0], color=next(ax._get_lines.prop_cycler)['color'], lw=2) for _ in range(25)]
    plt.legend(custom_lines, ['Naive MLP', 'Naive CNN', 'Simple', 'MLP', 'CNN', 'GloVe Simple', 'GloVe MLP', 'GloVe CNN', 'GloVe Train Simple', 'GloVe Train MLP', 'GloVe Train CNN'], loc='upper left')
    plt.show()

def plot_good_models_training():
    # Plot training & validation accuracy values
    ax = plt.gca()
    for result in [simple, mlp, cnn, glove_cnn, glove_train_simple, glove_train_mlp, glove_train_cnn]:
        color = next(ax._get_lines.prop_cycler)['color']
        plt.plot(result, color=color)
        # Show max
        y = max(result)
        x = result.index(y)
        plt.plot([x], [y], marker='o', markersize=10, color=color)
        ax.annotate(f"{y:.3f}", (x, y-0.01), ha='center', bbox=dict(boxstyle="round", fc="w", alpha=1))
    plt.ylim(0.5, 0.7)
    plt.xticks([epoch for epoch in range(0, 25)])
    plt.title('Model accuracy')
    plt.ylabel('Validation Accuracy')
    plt.xlabel('Epoch')
    plt.gca().set_prop_cycle(None)
    custom_lines = [Line2D([0], [0], color=next(ax._get_lines.prop_cycler)['color'], lw=2) for _ in range(25)]
    plt.legend(custom_lines, ['Simple', 'MLP', 'CNN', 'GloVe CNN', 'GloVe Train Simple', 'GloVe Train MLP', 'GloVe Train CNN'], loc='bottom right')
    plt.show()


plot_all_models_training()
plot_good_models_training()