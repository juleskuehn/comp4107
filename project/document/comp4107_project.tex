\documentclass[jou,apacite, 10px]{apa6}
\usepackage{listings}
\usepackage{courier}
\lstset{basicstyle=\small\ttfamily,breaklines=true}
\lstset{frame=single}
\makeatletter
\newcommand*{\rom}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother
\title{Classify a book's category based on its title}
\shorttitle{Book classification using book titles}

\twoauthors{Yunkai Wang}{Jules Kuehn}
\twoaffiliations{Carleton University, School of Computer Science}{Carleton University, School of Computer Science}

\abstract{Recently Convolutional neural networks(CNN) have been well-studied and be shown that they can archive incredible results on tasks like sentence classification (Kim, 2014). Another interesting topic that has been studied is if we can categorize a book based on its cover picture (Iwana et al.). However, as shown in the paper, they showed that they were able to draw a relationship between a book cover image, and its category, but the accuracy was less than $50\%$. Thus, we conduct this experiment to see if we can use CNN to learn the potential relationship between a book's title and its categories. For this project, we experiment the task with different model setups and layers using word embedding, and compare their accuracies.}

\leftheader{Jules, Yunkai}

\begin{document}
\maketitle    
                        
\section{\rom{1}. Introduction}
Book titles will given the reader a first impression of what the book may be about, and most of the time, a good book title will attract more readers from buying and reading the book. But what does the book title really tells you? What can you tell by simply looking at the book title? If a title contains the word 'calendar', then most likely people will all agree that it's a calendar, but what if the book title doesn't contain a specific word that reveals its category, like if you are given the book title 'The Three-Body Problem' with knowing this book before, how will you categorize the book? Will the book title be sufficient for a CNN that has been pre-trained with titles and categories be able to detect that as a science friction? This is a very interesting question to ask, and that's why we conduct this experiment, to see if the computers are able to learn the potential relationship between a book's title and its category using CNN.\\

\section{\rom{2}. Background/Related work}

\subsection{Dateset}
The \textit{Data Mining} is a dataset which can be found on \textit{Github.com}. The set consists of detailed information of $207572$  books from $32$ categories, to help determining the potential relationship between different information related to a book, like the relationship between a book's cover image and its category (Iwana et al.). Most of the categories are not similar to other categories, like 'calendar' and 'law', but some of the categories are pairwise similar, like 'Christian Books and Bibles' and 'Religion and Spirituality'. All books in the dataset have informations like the image url of the cover image of the book, the book's author, and the book's category. The dataset doesn't have the training dataset and testing set splitted by default, so we just use make a use of the \textit{sklearn} library, which provides a nice \textit{train\_test\_split} function to create the training set and test set.

\subsection{Loading and transforming the data}
Here are the list of things we did to load the dataset:

\rule{0pt}{4ex}  1. First we load all the original data line by line, and for each line, we extract the book title and its category, as those are the only things that we are feeding to our neural network.

\rule{0pt}{4ex}  2. It's obvious that titles all have different lengths, and the maximum title length is $96$. However, as Figure $1$ shows, most of titles have length less than $26$, only several titles have length $>26$. Therefore, we choose the maximum length to be $26$ instead of $96$. We used \textit{pad\_sequence} function from keras library to accomplish the task, which simply appends $0$ to the end of the titles that are shorter than the given length, and cut-off those long titles after $26$ words.

\rule{0pt}{4ex}  3. With the help of \textit{train\_test\_split} function, we split the training data and testing data by having a testing data of size $10\%$ of the original dataset.

\rule{0pt}{4ex}  4. We convert the titles into vector of integers, where each integer encodes to a word. We made use of the Keras' \textit{one\_hot} function to do the task. This vectorization process takes $\approx$ 2 hrs to run, therefore, we decide to store the vectorized data into a new .csv file, which we can use to train our data directly.\\

\section{\rom{3}. Problem Statement}

We want computers to be able to handle the categorization tasks for books based on book titles, such that in the future, the computer will be able to auto-categorize a new book given only its title. It's almost impossible to manually design a rule-based program, that will handle the categorization problem nicely and accurately. The titles and the categories don't have an obvious relationship which we can use to linearly separate the titles into categories. However, we can use CNN to learn the underlying relationship between the titles and the categories, that will achieve a high accuracy in the categorization task.\\

\section{\rom{4}. Model}
We began with 2 naive implementations, in which each "word vector" was an arbitrary unique integer. Each title is then a vector of these integers (one for each word), zero padded to the length of the longest title (96 words). This representation of the titles has little semantic value as the integers representing the words are arbitrary.

% Insert diagram of naive model
The first model (NaiveMLP) was a fully connected MLP with the following configuration. Output size of each layer is indicated in parentheses.

\begin{itemize}
    \item Input (96)
    \item FC Sigmoid (625)
    \item FC Sigmoid (300)
    \item FC Softmax (32)
\end{itemize}\rule{0pt}{4ex}
The second model (NaiveCNN) was based on a good model for the CIFAR-10 classification.

\begin{itemize}
    \item Input (96,1)
    \item Conv ReLU: 3 kernel, 1 stride (96,32)
    \item Conv ReLU: 3 kernel, 1 stride (96,32)
    \item Max pool: 2 kernel, 2 stride (48,32)
    \item Dropout: pKeep 0.8 (48,32)
    \item Conv ReLU: 3 kernel, 1 stride (48,64)
    \item Max pool: 2 kernel, 2 stride (24,64)
    \item Conv ReLU: 3 kernel, 1 stride (24,64)
    \item Max pool: 24 kernel, 24 stride (1,64)
    \item Dropout: pKeep 0.8 (1,64)
    \item Output Softmax (32)
\end{itemize}\rule{0pt}{4ex}
A better word representation is to embed each word in a n-dimensional space, where the position of each word reflects its meaning in relation to the other words. Each word is then represented by a dense vector of length n, with similar words having similar vectors. Similarity between words is inferred from context. Many pre-trained embeddings are available, trained through Word2Vec or GloVe on datasets pulled from Wikipedia, Twitter, or other sources. An embedding can also be trained from scratch, or initialized to a pre-trained dataset then trained further. We compare these three approaches.

The first embedding model (EmbedTrain) trained the embeddings from scratch, using only the book titles from the training set. Note that we truncated the titles from a maximum of 96 words to 26 words. We chose 32 for the embedding dimension, based on the rule of the thumb that the embedding should be the fourth root of the vocabulary size. There were roughly 70,000 words in the training dataset, which suggest that the embedding dimension should be 16, but we found that 32 offered a slight improvement in practice.

\begin{itemize}
    \item Input (26, 1)
    \item Embedding (26, 32)
    \item Flatten (832)
    \item Output Softmax (32)
\end{itemize}\rule{0pt}{4ex}
For each of the three embedding models, we also tested 2 variations to compare the performance of MLP vs CNN on the title embeddings. The first variation (Embed<Model>\_FC) adds a single fully connected (FC) layer:\\
\begin{itemize}
    \item Input (26, 1)
    \item Embedding (26, 32)
    \item Flatten (832)
    \item FC ReLU (512)
    \item Output Softmax (32)
\end{itemize}\rule{0pt}{4ex}
The second variation (Embed<Model>\_CNN) adds a single convolutional layer, followed by maxpool over the entire length:

\begin{itemize}
    \item Input (26, 1)
    \item Embedding (26, 32)
    \item Conv ReLU: 3 kernel, 1 stride (24, 512)
    \item Max pool: 24 kernel, 24 stride (1, 512)
    \item Flatten (512)
    \item Output Softmax (32)
\end{itemize}\rule{0pt}{4ex}
The second embedding model (EmbedGloveFixed) loaded 400,000 pre-trained 100-dimensional word-vectors from the GloVe.6B dataset. These word vectors were trained on Wikipedia and Gigaword, a newswire dataset. The model is identical to EmbedTrain except that the embeddings are pre-trained and fixed, with the embedding dimension increased to 100.

The third embedding model (EmbedGloveTrain) is identical to EmbedTrain except that the embeddings are initialized to the GloVe dataset as seen in EmbedGloveFixed. Embeddings are then retrained on the book titles.\\

\section{\rom{5}. Implementation}
The naive models were implemented using the course-provided cnn.py and mlp.py code. All other models were implemented in Keras, in which the code for the model is extremely concise:\\
\lstinputlisting{code/kerasModel.py}

\rule{0pt}{2ex}\section{\rom{6}. Experiment}
We did a couple of experiments using different models that we described in the model section. Here are the results of running these experiments.

\subsection{Multiple layer perceptron(mlp.py)}
With the original mlp.py file, and the size of input layer and output layer  changed to match what we needed, the accuracy was only $\approx$9\%. This simply concludes that MLP will not be able to handle this classification task, since this data set is not like MNIST, two book titles from the same category may be very different, and classifying such dataset is not something MLP is good at.

\subsection {Convolutional neural network(cnn.py)}
Using the original cnn.py with changed input size and output size, and it turned out that the accuracy was slightly higher than the accuracy using the MLP($\approx$12\%), but it was still really bad at this task.



\section{\rom{7}. Conclusion}

In this project, we experimented using different neural network models to categorize book based on its title, and we archived a better result compared with the accuracies if the neural network judge the book based on its cover image (Iwana et al.). We have shown that with word embedding and a very simple CNN, we are able to categorize the books well, even though it's not very accurate for some of the titles. However, since the neural network is very small, we are able to train the network within a small speed, which makes it possible to train the network with a larger dataset in order to improve the result, especially feeding more training datas for those similar categories within a reasonable small time. For future improvement, we can try to find a more complicated NN, but that will definitely make it harder to train the network with large data set because of the computational complexity. It's also possible to dropout some of the input words, when these words are the most common words that will cause a book title to be incorrectly categorized, but that's beyond the scope of this project.

The \textit{Data Mining} dataset is an interesting dataset to be working on, as it provides the capability to conducting many different possible classification experiments, like the relationship between a book title and the book author, etc. However, if the original dataset can provide a typical sentence from each book (like the first sentence from the book or the best sentence from the book), then we can conduct even more interesting experiments. Also, it will be helpful if people can enrich that dataset with more books that belong those small sized categories like Education \& training, which will definitely improve the classification accuracy of our model, and possibly the others.

\section{References}
\noindent Convolutional Neural Networks for Sentence Classification By Yoon Kim\\
Judging a Book by its Cover By Brian et al.

\end{document}
